{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI and Machine Learning Coursework\n",
    "\n",
    "Use the provided retinal image datasets (classified as normal, cataract or glaucoma) to develop a machine learning model that predicts the disease status of an image. Apply your knowledge from previous workshops to design, implement, and evaluate the model.\n",
    "\n",
    "This was done in Python 3.11.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASIC PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "import sys\n",
    "# File path for retina images\n",
    "path = 'path/Images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make file paths for each image class\n",
    "Normal = os.path.join(path,'1_normal')\n",
    "Cataract = os.path.join(path,'2_cataract')\n",
    "Glaucoma = os.path.join(path,'2_glaucoma')\n",
    "# print out how many images are in each class\n",
    "print(\"Number of normal retinas\", len(os.listdir(Normal)))\n",
    "print(\"Number of cataract retinas\", len(os.listdir(Cataract)))\n",
    "print(\"Number of glaucoma retinas\", len(os.listdir(Glaucoma)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for loading images\n",
    "def load_images(directories, n_images=900000):\n",
    "    \"\"\"\n",
    "    Reads in images and assigns class labels\n",
    "    Parameters:\n",
    "        directories: A list of the sub-directories\n",
    "        n_images:    The maximum number of images to load from each directory\n",
    "    Returns:\n",
    "        images (numpy.ndarray) : Image data\n",
    "        label (numpy.ndarray      : Labels of each image\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label, sub_dir in enumerate(directories):\n",
    "        num=1\n",
    "        for file_name in os.listdir(sub_dir):\n",
    "            if num > n_images:\n",
    "                break\n",
    "            img_path = os.path.join(sub_dir, file_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, (500, 500))  # Resize to a smaller, consistent shape\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "                num+=1\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load images into python\n",
    "images, labels = load_images([Normal, Cataract, Glaucoma], 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to make sure images have properly loaded in\n",
    "print(images.shape)\n",
    "# display an image to see if it has loaded in correctly\n",
    "plt.imshow(images[296],cmap=\"gray\")\n",
    "#python counts from 0 so image 297 in folder is actually 296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test data, 20% is testing data with a random seed set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import DenseNet model\n",
    "from tensorflow.keras.applications import DenseNet169\n",
    "#using Resnet101V2 model\n",
    "densenet_model = DenseNet169(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape= (500,500,3),\n",
    ")\n",
    "\n",
    "# freeze the ResNet convolutional layers to add extra trainable layers to the ResNet NN so we can train on our data\n",
    "\n",
    "for layer in densenet_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup early stopping after 5 epochs of no improvement to loss\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "callback = tensorflow.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    restore_best_weights=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Flatten, Dense, Rescaling, Input, RandAugment\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "initializer = HeNormal()\n",
    "\n",
    "model = Sequential( [\n",
    " Input(shape=(500, 500, 3)),\n",
    " tensorflow.keras.layers.Rescaling(1./255),\n",
    " tensorflow.keras.layers.RandomContrast(factor=0.1, value_range= [0,255]),\n",
    " #tensorflow.keras.layers.RandomBrightness(factor=0.2, value_range= [0,255]), #this augmentation massively decreases performance for some reason, probably configured incorrectly\n",
    " tensorflow.keras.layers.RandomFlip(\"horizontal\"),\n",
    " tensorflow.keras.layers.RandomRotation((-0.1,0.1)), # % of 2*PI radians rotation \n",
    " tensorflow.keras.layers.RandomZoom(0.1),\n",
    " tensorflow.keras.layers.RandomTranslation(height_factor=(-0.1,0.1), width_factor=(-0.1,0.1)),\n",
    " densenet_model, \n",
    " Flatten(),\n",
    " Dense(128, activation='relu'),  # Put into a dense layer size\n",
    " Dense(64, activation='relu'),  # Put into a dense layer size\n",
    " Dense(3, activation='softmax')  # 3 state classification - gives probability of belonging to each class\n",
    "] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "#compile model\n",
    "model.compile(optimizer=Adamax(learning_rate= 0.001),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy', 'categorical_accuracy'])\n",
    "# Categorical Crossentropy is designed for multi-class classifications with multiple class outputs.\n",
    "# print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE-Tomek Links Over and undersampling\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "#need to reshape data to work with resampling functions\n",
    "reshaped_X = X_train.reshape(X_train.shape[0],-1)\n",
    "\n",
    "#resampling with SMOTE and Tomek links\n",
    "smote_object = SMOTE(sampling_strategy='not majority', random_state=69, k_neighbors=5)\n",
    "tomek_object = TomekLinks(sampling_strategy = 'majority')\n",
    "resample = SMOTETomek(sampling_strategy = 'not majority', smote = smote_object, tomek = tomek_object, random_state = 69 )\n",
    "resampled_X, resampled_y  = resample.fit_resample(reshaped_X , y_train)\n",
    "\n",
    "# reshaping back to the initial dimensions\n",
    "new_X_train = resampled_X.reshape(-1,500,500,3)\n",
    "new_y_train = resampled_y.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "# one hot encoding for categorical crossentropy\n",
    "y_train_cat = tensorflow.keras.utils.to_categorical(new_y_train, num_classes=3)\n",
    "# also for the test set\n",
    "y_test_cat = tensorflow.keras.utils.to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that the one hot encoding and resampling worked \n",
    "y_train_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training\n",
    "history = model.fit(x=new_X_train, y=y_train_cat, batch_size=50,\n",
    "                      epochs=30, shuffle=True, \n",
    "                      validation_split=0.2, callbacks = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optionally save the model for later\n",
    "model.save('path_to_directory/Retina_model2.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CROSS VALIDATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optionally load in the saved pretrained model and its weights if you somehow lost the saved model before\n",
    "model = tensorflow.keras.models.load_model('path_to_saved_model/Retina_model2.keras')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation with Stratified K-fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "#setup cross validation object\n",
    "validate = StratifiedKFold(n_splits=5, shuffle= True)\n",
    "#tensorflow/keras model needs to be wrapped for sklearn to process it\n",
    "keras_model = KerasClassifier(model= model, loss= 'categorical_crossentropy', optimizer='Adamax', epochs = 30, batch_size= 50)\n",
    "#deisgnate metrics to evaluate\n",
    "metrics = {'acc': 'accuracy',\n",
    "           'prec_macro': 'precision_macro', 'prec_weight': 'precision_weighted',\n",
    "           'f1_macro':'f1_macro', 'f1_weight': 'f1_weighted', \n",
    "           'recall_macro':'recall_macro', 'recall_weight': 'recall_weighted',\n",
    "           'roc': 'roc_auc_ovr', 'roc_weighted': 'roc_auc_ovr_weighted'}\n",
    "\n",
    "#run cross validation (can take several hours, very memory intensive)\n",
    "scores = cross_validate(keras_model, images, labels, scoring= metrics, cv=validate, n_jobs=5, error_score=\"raise\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Loss graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy changes during training as a graph\n",
    "epochs = history.epoch\n",
    "accuracy_values = history.history['accuracy']\n",
    "val_accuracy_values = history.history['val_accuracy']\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(epochs, accuracy_values, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracy_values, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss changes during training as a graph\n",
    "loss_values = history.history['loss']\n",
    "val_loss_values = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(epochs, loss_values, 'b', label='Training Loss')\n",
    "plt.plot(epochs, val_loss_values, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to make predictions on the testset\n",
    "y_pred_prob = model.predict(X_test)\n",
    "# Convert predictions to binary class labels \n",
    "y_pred  = y_pred_prob.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make confusion matrix from predictions\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm  = confusion_matrix(y_test, y_pred)\n",
    "cmdisp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "cmdisp.plot(include_values=True, cmap=\"viridis\", ax=ax, xticks_rotation=\"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print classification table\n",
    "from sklearn.metrics  import classification_report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data needs to be transformed to work with the plots\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer().fit(y_train)\n",
    "y_onehot_test = label_binarizer.transform(y_test)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate PR curves for each class\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "n_classes = 3\n",
    "\n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_onehot_test[:, i], y_pred_prob[:, i])\n",
    "    average_precision[i] = average_precision_score(y_onehot_test[:, i], y_pred_prob[:, i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setup plot details\n",
    "colors = cycle([\"navy\", \"turquoise\", \"darkorange\", \"cornflowerblue\", \"teal\"])\n",
    "\n",
    "_, ax = plt.subplots(figsize=(7, 8))\n",
    "\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines, labels = [], []\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    display = PrecisionRecallDisplay(\n",
    "        recall=recall[i],\n",
    "        precision=precision[i],\n",
    "        average_precision=average_precision[i],\n",
    "    )\n",
    "    display.plot(\n",
    "        ax=ax, name=f\"Precision-recall for class {i}\", color=color\n",
    "    )\n",
    "\n",
    "# add the legend for the iso-f1 curves\n",
    "handles, labels = display.ax_.get_legend_handles_labels()\n",
    "handles.extend([l])\n",
    "# set the legend and the axes\n",
    "ax.legend(handles=handles, labels=labels, loc=\"best\")\n",
    "ax.set_title(\"Precision-Recall curve of every Retina Image class\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup objects beforehand\n",
    "fpr, tpr, roc_auc = dict(), dict(), dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate macro averaged ROC AUC score\n",
    "macro_roc_auc_ovr = roc_auc_score(\n",
    "    y_test,\n",
    "    y_pred_prob,\n",
    "    multi_class=\"ovr\",\n",
    "    average=\"macro\",\n",
    ")\n",
    "\n",
    "print(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{macro_roc_auc_ovr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate micro averaged ROC AUC score\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_onehot_test.ravel(), y_pred_prob.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "print(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['micro']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot ROC AUC graph \n",
    "from itertools import cycle\n",
    "n_classes = 3\n",
    "target_names = ('Normal', 'Cataract', 'Glaucoma')\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "#micro ROC AUC curve\n",
    "plt.plot(\n",
    "    fpr[\"micro\"],\n",
    "    tpr[\"micro\"],\n",
    "    label=f\"micro-average ROC curve (AUC = {roc_auc['micro']:.2f})\",\n",
    "    color=\"deeppink\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "#macro ROC AUC curve\n",
    "plt.plot(\n",
    "    fpr[\"macro\"],\n",
    "    tpr[\"macro\"],\n",
    "    label=f\"macro-average ROC curve (AUC = {roc_auc['macro']:.2f})\",\n",
    "    color=\"navy\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "#ROC AUC curve for each class\n",
    "colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "for class_id, color in zip(range(n_classes), colors):\n",
    "    RocCurveDisplay.from_predictions(\n",
    "        y_onehot_test[:, class_id],\n",
    "        y_pred_prob[:, class_id],\n",
    "        name=f\"ROC curve for {target_names[class_id]}\",\n",
    "        color=color,\n",
    "        ax=ax,\n",
    "        plot_chance_level=(class_id == 2),\n",
    "    )\n",
    "#setting axis labels and titles\n",
    "_ = ax.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Receiver Operating Characteristic\\nto One-vs-Rest multiclass of Retina Images\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
