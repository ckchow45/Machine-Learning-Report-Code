{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI and Machine Learning Coursework Experimental Code\n",
    "\n",
    "Use the provided retinal image datasets (classified as normal, cataract or glaucoma) to develop a machine learning model that predicts the disease status of an image. Apply your knowledge from previous workshops to design, implement, and evaluate the model.\n",
    "\n",
    "This notebook contains experimental code and techniques that were tested, but not used, either due to poor performance, technical issues or lack of computational power/time\n",
    "\n",
    "This was done in Python 3.11.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASIC PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "import sys\n",
    "# File path for retina images\n",
    "path = 'path/Images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make file paths for each image class\n",
    "Normal = os.path.join(path,'1_normal')\n",
    "Cataract = os.path.join(path,'2_cataract')\n",
    "Glaucoma = os.path.join(path,'2_glaucoma')\n",
    "# print out how many images are in each class\n",
    "print(\"Number of normal retinas\", len(os.listdir(Normal)))\n",
    "print(\"Number of cataract retinas\", len(os.listdir(Cataract)))\n",
    "print(\"Number of glaucoma retinas\", len(os.listdir(Glaucoma)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for loading images\n",
    "def load_images(directories, n_images=900000):\n",
    "    \"\"\"\n",
    "    Reads in images and assigns class labels\n",
    "    Parameters:\n",
    "        directories: A list of the sub-directories\n",
    "        n_images:    The maximum number of images to load from each directory\n",
    "    Returns:\n",
    "        images (numpy.ndarray) : Image data\n",
    "        label (numpy.ndarray      : Labels of each image\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label, sub_dir in enumerate(directories):\n",
    "        num=1\n",
    "        for file_name in os.listdir(sub_dir):\n",
    "            if num > n_images:\n",
    "                break\n",
    "            img_path = os.path.join(sub_dir, file_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, (500, 500))  # Resize to a smaller, consistent shape\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "                num+=1\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load images into python\n",
    "images, labels = load_images([Normal, Cataract, Glaucoma], 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to make sure images have properly loaded in\n",
    "print(images.shape)\n",
    "# display an image to see if it has loaded in correctly\n",
    "plt.imshow(images[296],cmap=\"gray\")\n",
    "#python counts from 0 so image 297 in folder is actually 296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test data, 20% is testing data with a random seed set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import DenseNet model\n",
    "from tensorflow.keras.applications import DenseNet169\n",
    "#using Resnet101V2 model\n",
    "densenet_model = DenseNet169(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape= (500,500,3),\n",
    ")\n",
    "\n",
    "# freeze the ResNet convolutional layers to add extra trainable layers to the ResNet NN so we can train on our data\n",
    "\n",
    "for layer in densenet_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup early stopping after 5 epochs of no improvement to loss\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "callback = tensorflow.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    restore_best_weights=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Flatten, Dense, Rescaling, Input, RandAugment\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "initializer = HeNormal()\n",
    "\n",
    "model = Sequential( [\n",
    " Input(shape=(500, 500, 3)),\n",
    " tensorflow.keras.layers.Rescaling(1./255),\n",
    " tensorflow.keras.layers.RandomContrast(factor=0.1, value_range= [0,255]),\n",
    " #tensorflow.keras.layers.RandomBrightness(factor=0.2, value_range= [0,255]), #this augmentation massively decreases performance for some reason, probably configured incorrectly\n",
    " tensorflow.keras.layers.RandomFlip(\"horizontal\"),\n",
    " tensorflow.keras.layers.RandomRotation((-0.1,0.1)), # % of 2*PI radians rotation \n",
    " tensorflow.keras.layers.RandomZoom(0.1),\n",
    " tensorflow.keras.layers.RandomTranslation(height_factor=(-0.1,0.1), width_factor=(-0.1,0.1)),\n",
    " densenet_model, \n",
    " Flatten(),\n",
    " Dense(128, activation='relu'),  # Put into a dense layer size\n",
    " Dense(64, activation='relu'),  # Put into a dense layer size\n",
    " Dense(3, activation='softmax')  # 3 state classification - gives probability of belonging to each class\n",
    "] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate class weights for categorical focal crossentropy\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "y_unique =np.unique(y_train)\n",
    "alpha_set = compute_class_weight(class_weight=\"balanced\", classes=y_unique, y=y_train)\n",
    "alpha_set = np.asarray(alpha_set).astype('float64')\n",
    "\n",
    "#put class weights into dictionary for later use in model training\n",
    "class_weights = {0:alpha_set[0], 1:alpha_set[1], 2:alpha_set[2]}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.losses import CategoricalFocalCrossentropy\n",
    "\n",
    "#compile model with categorical focal crossentropy\n",
    "model.compile(optimizer=Adamax(learning_rate= 0.001),\n",
    "                loss=tensorflow.keras.losses.CategoricalFocalCrossentropy(alpha = alpha_set, reduction='sum_over_batch_size', gamma=2),\n",
    "                metrics=['accuracy', 'categorical_accuracy'])\n",
    "# Categorical Focal Crossentropy is tailored for multi-class scenarios with imbalanced datasets and multiple outputs\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "# one hot encoding for categorical crossentropy\n",
    "y_train_cat = tensorflow.keras.utils.to_categorical(new_y_train, num_classes=3)\n",
    "# also for the test set\n",
    "y_test_cat = tensorflow.keras.utils.to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVER/UNDERSAMPLING CODE BLOCKS\n",
    "\n",
    "Pick one code block to use or do not pick any at all \n",
    "\n",
    "The code below is the different resampling methods I tested before settling on a combination of SMOTE and Tomek Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OVERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "#imblearn function only works with 2D arrays inputs, so reshape data to fit\n",
    "reshaped_X = X_train.reshape(X_train.shape[0],-1)\n",
    "\n",
    "#oversampling with imblearn function\n",
    "oversample = RandomOverSampler()\n",
    "oversampled_X, oversampled_y  = oversample.fit_resample(reshaped_X , y_train)\n",
    "\n",
    "# reshaping X back to the initial dimensions\n",
    "new_X_train = oversampled_X.reshape(-1,300,300,3)\n",
    "new_y_train = oversampled_y.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversampling with SMOTE method\n",
    "from imblearn.over_sampling import SMOTE\n",
    "#imblearn function only works with 2D arrays inputs, so reshape data to fit\n",
    "reshaped_X = X_train.reshape(X_train.shape[0],-1)\n",
    "\n",
    "\n",
    "#oversampling with imblearn function\n",
    "oversample = SMOTE(sampling_strategy='auto', random_state=69, k_neighbors=5)\n",
    "oversampled_X, oversampled_y  = oversample.fit_resample(reshaped_X , y_train)\n",
    "\n",
    "# reshaping X back to the initial dimensions\n",
    "new_X_train = oversampled_X.reshape(-1,500,500,3)\n",
    "new_y_train = oversampled_y.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversampling with ADASYN method\n",
    "from imblearn.over_sampling import ADASYN\n",
    "#imblearn function only works with 2D arrays inputs, so reshape data to fit\n",
    "reshaped_X = X_train.reshape(X_train.shape[0],-1)\n",
    "\n",
    "#oversampling with imblearn function\n",
    "oversample = ADASYN(sampling_strategy='auto', random_state=69, n_neighbors=5)\n",
    "oversampled_X, oversampled_y  = oversample.fit_resample(reshaped_X , y_train)\n",
    "\n",
    "# reshaping X back to the initial dimensions\n",
    "new_X_train = oversampled_X.reshape(-1,500,500,3)\n",
    "new_y_train = oversampled_y.reshape(-1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNDERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random undersampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "reshaped_X = X_train.reshape(X_train.shape[0],-1)\n",
    "\n",
    "#undersampling \n",
    "undersample = RandomUnderSampler()\n",
    "undersampled_X, undersampled_y  = undersample.fit_resample(reshaped_X , y_train)\n",
    "\n",
    "# reshaping X back to the first dims\n",
    "new_X_train = undersampled_X.reshape(-1,300,300,3)\n",
    "new_y_train = undersampled_y.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python 3.11\\Lib\\site-packages\\imblearn\\under_sampling\\_prototype_selection\\_nearmiss.py:206: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Near miss undersampling\n",
    "from imblearn.under_sampling import NearMiss\n",
    "reshaped_X = X_train.reshape(X_train.shape[0],-1)\n",
    "\n",
    "#undersampling\n",
    "undersample = NearMiss(version=3, n_neighbors=3)\n",
    "undersampled_X, undersampled_y  = undersample.fit_resample(reshaped_X , y_train)\n",
    "\n",
    "# reshaping X back to the first dims\n",
    "new_X_train = undersampled_X.reshape(-1,500,500,3)\n",
    "new_y_train = undersampled_y.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tomek Links undersampling\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "reshaped_X = X_train.reshape(X_train.shape[0],-1)\n",
    "\n",
    "#undersampling\n",
    "undersample = TomekLinks(sampling_strategy = 'majority')\n",
    "undersampled_X, undersampled_y  = undersample.fit_resample(reshaped_X , y_train)\n",
    "\n",
    "# reshaping X back to the first dims\n",
    "new_X_train = undersampled_X.reshape(-1,300,300,3)\n",
    "new_y_train = undersampled_y.reshape(-1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMBINATION OF OVER AND UNDERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE-Tomek Links Over and undersampling\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "reshaped_X = X_train.reshape(X_train.shape[0],-1)\n",
    "\n",
    "#resampling with SMOTE and Tomek links\n",
    "smote_object = SMOTE(sampling_strategy='not majority', random_state=69, k_neighbors=5)\n",
    "tomek_object = TomekLinks(sampling_strategy = 'majority')\n",
    "resample = SMOTETomek(sampling_strategy = 'not majority', smote = smote_object, tomek = tomek_object, random_state = 69 )\n",
    "resampled_X, resampled_y  = resample.fit_resample(reshaped_X , y_train)\n",
    "\n",
    "# reshaping X back to the first dims\n",
    "new_X_train = resampled_X.reshape(-1,500,500,3)\n",
    "new_y_train = resampled_y.reshape(-1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the new resampled X and Y train were taken out from the code below and the old X and Y training data was used as I wanted to test the new loss function and class weights. The new model did not perform particularly well however, tending to be biased for glaucoma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "# one hot encoding for categorical crossentropy\n",
    "y_train_cat = tensorflow.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "# also for the test set\n",
    "y_test_cat = tensorflow.keras.utils.to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that the one hot encoding and resampling worked \n",
    "new_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training\n",
    "history = model.fit(x=X_train, y=y_train_cat, batch_size=50,\n",
    "                      epochs=30, shuffle=True, \n",
    "                      validation_split=0.2, callbacks = callback, class_weight = class_weights)\n",
    "#class weights were added into the model training in order to get it to focus on the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optionally save the model for later\n",
    "model.save('path_to_directory/Retina_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CROSS VALIDATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optionally load in the saved pretrained model and its weights if you somehow lost the saved model before\n",
    "model = tensorflow.keras.models.load_model('path_to_saved_model/Retina_model.keras')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation with Stratified K-fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "#setup cross validation object\n",
    "validate = StratifiedKFold(n_splits=5, shuffle= True)\n",
    "#tensorflow/keras model needs to be wrapped for sklearn to process it\n",
    "keras_model = KerasClassifier(model= model, loss= 'categorical_focal_crossentropy', optimizer='Adamax', epochs = 30, batch_size= 50)\n",
    "#deisgnate metrics to evaluate\n",
    "metrics = {'acc': 'accuracy',\n",
    "           'prec_macro': 'precision_macro', 'prec_weight': 'precision_weighted',\n",
    "           'f1_macro':'f1_macro', 'f1_weight': 'f1_weighted', \n",
    "           'recall_macro':'recall_macro', 'recall_weight': 'recall_weighted',\n",
    "           'roc': 'roc_auc_ovr', 'roc_weighted': 'roc_auc_ovr_weighted'}\n",
    "\n",
    "#run cross validation (can take several hours, very memory intensive)\n",
    "scores = cross_validate(keras_model, images, labels, scoring= metrics, cv=validate, n_jobs=5, error_score=\"raise\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for some reason cross validation does not run with categorical focal crossentropy, the main error seems to be that the cross validation needs to convert the alpha values into a tensor? and is unable to do that, probably something to do with the wrapper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPARAMTER OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization with RandomSearch and Stratified K-fold\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "#prepare cross validation method\n",
    "cv = StratifiedKFold(n_splits=3, shuffle = True, random_state=69)\n",
    "# need to wrap the Keras model in a way that scikit learn can recognise\n",
    "keras_model = KerasClassifier(model= model, loss= 'categorical_crossentropy', optimizer='adamax', epochs = 30, batch_size= 50, class_weight = class_weights)\n",
    "#parameter grid/dictionary setup for hyperparameter testing\n",
    "parameters =  { 'epochs': [20,50], 'batch_size': [25,75]}\n",
    "search = RandomizedSearchCV(estimator = keras_model, param_distributions= parameters, cv=cv, scoring='f1_weighted', n_jobs=4, verbose = 1, error_score='raise')\n",
    "\n",
    "results = search.fit(X_train, y_train)\n",
    "results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of available parameters that can be tested\n",
    "keras_model.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter optimization was very computationally intensive, this was run on the HPC for almost 48 hours before finishing so it was decided to not use it.\n",
    "\n",
    "Running this locally often caused the kernel to crash or for I/O operation errors and memory issues to terminate the process\n",
    "\n",
    "The parameters it can test also appear to either be very limited or very finicky to setup as I could not get it to test different learning rates or different gamma values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Loss graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy changes during training as a graph\n",
    "epochs = history.epoch\n",
    "accuracy_values = history.history['accuracy']\n",
    "val_accuracy_values = history.history['val_accuracy']\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(epochs, accuracy_values, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracy_values, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss changes during training as a graph\n",
    "loss_values = history.history['loss']\n",
    "val_loss_values = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(epochs, loss_values, 'b', label='Training Loss')\n",
    "plt.plot(epochs, val_loss_values, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to make predictions on the testset\n",
    "y_pred_prob = model.predict(X_test)\n",
    "# Convert predictions to binary class labels \n",
    "y_pred  = y_pred_prob.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make confusion matrix from predictions\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm  = confusion_matrix(y_test, y_pred)\n",
    "cmdisp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "cmdisp.plot(include_values=True, cmap=\"viridis\", ax=ax, xticks_rotation=\"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print classification table\n",
    "from sklearn.metrics  import classification_report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data needs to be transformed to work with the plots\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "label_binarizer = LabelBinarizer().fit(y_train)\n",
    "y_onehot_test = label_binarizer.transform(y_test)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate PR curves for each class\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "n_classes = 3\n",
    "\n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_onehot_test[:, i], y_pred_prob[:, i])\n",
    "    average_precision[i] = average_precision_score(y_onehot_test[:, i], y_pred_prob[:, i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setup plot details\n",
    "colors = cycle([\"navy\", \"turquoise\", \"darkorange\", \"cornflowerblue\", \"teal\"])\n",
    "\n",
    "_, ax = plt.subplots(figsize=(7, 8))\n",
    "\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines, labels = [], []\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    display = PrecisionRecallDisplay(\n",
    "        recall=recall[i],\n",
    "        precision=precision[i],\n",
    "        average_precision=average_precision[i],\n",
    "    )\n",
    "    display.plot(\n",
    "        ax=ax, name=f\"Precision-recall for class {i}\", color=color\n",
    "    )\n",
    "\n",
    "# add the legend for the iso-f1 curves\n",
    "handles, labels = display.ax_.get_legend_handles_labels()\n",
    "handles.extend([l])\n",
    "# set the legend and the axes\n",
    "ax.legend(handles=handles, labels=labels, loc=\"best\")\n",
    "ax.set_title(\"Precision-Recall curve of every Retina Image class\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup objects beforehand\n",
    "fpr, tpr, roc_auc = dict(), dict(), dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate macro averaged ROC AUC score\n",
    "macro_roc_auc_ovr = roc_auc_score(\n",
    "    y_test,\n",
    "    y_pred_prob,\n",
    "    multi_class=\"ovr\",\n",
    "    average=\"macro\",\n",
    ")\n",
    "\n",
    "print(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{macro_roc_auc_ovr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate micro averaged ROC AUC score\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_onehot_test.ravel(), y_pred_prob.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "print(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['micro']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot ROC AUC graph \n",
    "from itertools import cycle\n",
    "n_classes = 3\n",
    "target_names = ('Normal', 'Cataract', 'Glaucoma')\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "#micro ROC AUC curve\n",
    "plt.plot(\n",
    "    fpr[\"micro\"],\n",
    "    tpr[\"micro\"],\n",
    "    label=f\"micro-average ROC curve (AUC = {roc_auc['micro']:.2f})\",\n",
    "    color=\"deeppink\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "#macro ROC AUC curve\n",
    "plt.plot(\n",
    "    fpr[\"macro\"],\n",
    "    tpr[\"macro\"],\n",
    "    label=f\"macro-average ROC curve (AUC = {roc_auc['macro']:.2f})\",\n",
    "    color=\"navy\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "#ROC AUC curve for each class\n",
    "colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "for class_id, color in zip(range(n_classes), colors):\n",
    "    RocCurveDisplay.from_predictions(\n",
    "        y_onehot_test[:, class_id],\n",
    "        y_pred_prob[:, class_id],\n",
    "        name=f\"ROC curve for {target_names[class_id]}\",\n",
    "        color=color,\n",
    "        ax=ax,\n",
    "        plot_chance_level=(class_id == 2),\n",
    "    )\n",
    "#setting axis labels and titles\n",
    "_ = ax.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Receiver Operating Characteristic\\nto One-vs-Rest multiclass of Retina Images\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
